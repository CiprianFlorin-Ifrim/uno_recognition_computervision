{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d5f1d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nReferences:\\nhttps://www.pyimagesearch.com/2021/01/19/opencv-bitwise-and-or-xor-and-not/\\nhttps://www.pyimagesearch.com/2015/04/06/zero-parameter-automatic-canny-edge-detection-with-python-and-opencv/\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "References:\n",
    "https://www.pyimagesearch.com/2021/01/19/opencv-bitwise-and-or-xor-and-not/\n",
    "https://www.pyimagesearch.com/2015/04/06/zero-parameter-automatic-canny-edge-detection-with-python-and-opencv/\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5691ef29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2                                                                                             #import OpenCV2 library for image processing and algorithms\n",
    "import math                                                                                            #import python's math library\n",
    "import pickle                                                                                          #import the pickle library to save and load classifiers\n",
    "import operator                                                                                        #additional efficient pyhton fucntions\n",
    "import numpy as np                                                                                     #import numpy mathematical library \n",
    "import matplotlib.pyplot as plt                                                                        #import matplotlib library for plotting                    \n",
    "from IPython.core.display import display, HTML                                    \n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))                                  #change width of Jupyer Notebook to use the whole window resolution available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe19d793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------DEFINITIONS-------------------------------------------------------------------------------------------\n",
    "def map_value(value, in_low, in_high, out_low, out_high):                                              #create Arduino map() function in python for usage throughout the code\n",
    "    return out_low + (out_high - out_low) * ((value - in_low) / (in_high - in_low))                    #scale input lowest,input highest range to output lowest,output highest range then return\n",
    "\n",
    "def to_bw(image):\n",
    "    output = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    return output\n",
    "\n",
    "def rotate_img(img, angle):\n",
    "    result = ndimage.rotate(img, angle) \n",
    "    return result \n",
    "\n",
    "def find_contours(img_cnt, method, sigma = 0.33):\n",
    "    img_cnt_bw = to_bw(img_cnt)                                                             #convert rescaled center image to black and white channels for post-processing\n",
    "    blurred = cv2.GaussianBlur(img_cnt_bw, (3, 3), 0)\n",
    "    th_cnt_value, th_cnt_img = cv2.threshold(blurred,100,200,cv2.THRESH_OTSU)                    #accurate countours, smoother edges compared to regular binary\n",
    "    kernel_close = np.ones((3, 3), np.uint8)                                                               #higher kernel = less accurate contours\n",
    "    morph_cnt_img = cv2.morphologyEx(th_cnt_img, cv2.MORPH_CLOSE, kernel_close)                       #erosion + dilute method (internal spaces removal)\n",
    "    \n",
    "    v = np.median(morph_cnt_img)\n",
    "    lower = int(max(0, (1.0 - sigma) * v))\n",
    "    upper = int(min(255, (1.0 + sigma) * v))\n",
    "    edged = cv2.Canny(morph_cnt_img, lower, upper) \n",
    "    \n",
    "    if method == \"EXTERNAL\":\n",
    "        contours_img_cnt, hierarchy_img_cnt = cv2.findContours(edged, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) \n",
    "    elif method == \"TREE\":\n",
    "        contours_img_cnt, hierarchy_img_cnt = cv2.findContours(edged, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE) \n",
    "    return contours_img_cnt, hierarchy_img_cnt, edged\n",
    "\n",
    "def contour_removal_properties(image, min_corners, max_corners, min_ratio, max_ratio, min_height, max_height):\n",
    "    contours_found, __ = cv2.findContours(image, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE) \n",
    "    for c, cnt in enumerate(contours_found):                                                           # loop through all the found contours\n",
    "        perimeter, area, corners = contour_properties(cnt)\n",
    "        height, width = shape_height_width(cnt)\n",
    "        if not min_corners < corners < max_corners or min_ratio < area/perimeter < max_ratio or min_height < height < max_height:\n",
    "            contours_found.pop(c) \n",
    "    return contours_found\n",
    "\n",
    "def contour_removal_distance_centerpoints(contour_space, contour_space_center_x, contour_space_center_y, dist_min):\n",
    "    for i, cnt in enumerate(contour_space):\n",
    "        M = cv2.moments(cnt)\n",
    "        cx = int(M['m10']/M['m00'])\n",
    "        cy = int(M['m01']/M['m00'])\n",
    "        if ((((cx - contour_space_center_x)**2) + ((contour_space_center_y)**2))**0.5) > dist_min:\n",
    "            contours_space.pop(i)\n",
    "    return contour_space\n",
    "\n",
    "def draw_selected_contour(original, contour_space, contour, R,G,B, thickness):\n",
    "    drawn_contours_img = cv2.drawContours(original, contour_space[contour], -1, (R,G,B), thickness, cv2.LINE_AA)\n",
    "    return drawn_contours_img\n",
    "\n",
    "def yougest_children_img(img, contours_space_cnt, hierarchy_space_cnt):\n",
    "    hierarchy_space_cnt = hierarchy_space_cnt[0]\n",
    "    hierarchy_children_img = img.copy()\n",
    "    children_found = 0\n",
    "    for component in zip(contours_space_cnt, hierarchy_space_cnt):\n",
    "        currentContour = component[0]\n",
    "        currentHierarchy = component[1]\n",
    "        x,y,w,h = cv2.boundingRect(currentContour)\n",
    "        pixel_b, pixel_g, pixel_r = img[int((x+w)/2) ][int((y+h)/2)] \n",
    "        pixel_bgr = [pixel_b, pixel_g, pixel_r]\n",
    "        if pixel_bgr != [241, 235, 230]:\n",
    "            if currentHierarchy[2] < 0:                                                                            # these are the innermost child components            \n",
    "                cv2.rectangle(hierarchy_children_img ,(x,y),(x+w,y+h),(0,0,255),3)\n",
    "                children_found += 1\n",
    "    return int(children_found/3), hierarchy_children_img\n",
    "\n",
    "def find_center(image):\n",
    "    contour_bw = to_bw(image)                                                              #convert rescaled center image to black and white channels for post-processing\n",
    "    thr_value2, th_shape_img = cv2.threshold(contour_bw,127,255,cv2.THRESH_OTSU)                    #accurate countours, smoother edges compared to regular binary\n",
    "    kernel_close = np.ones((3, 3), np.uint8)                                                               #higher kernel = less accurate contours\n",
    "    morph_shape_img = cv2.morphologyEx(th_shape_img , cv2.MORPH_CLOSE, kernel_close)                       #erosion + dilute method (internal spaces removal)\n",
    "    canny_shape_img = cv2.Canny(morph_shape_img, 150, 500)  \n",
    "    contours_shape, hierarchy_shape = cv2.findContours(canny_shape_img, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE) \n",
    "\n",
    "    center_identified_img = image.copy()\n",
    "    M = cv2.moments(contours_shape[1])\n",
    "    if M['m00'] != 0:\n",
    "        cx = int(M['m10']/M['m00'])\n",
    "        cy = int(M['m01']/M['m00'])\n",
    "        cv2.circle(center_identified_img, (cx, cy), 7, (0, 0, 255), -1)\n",
    "        cv2.putText(center_identified_img, \"CENTER\", (cx - 30, cy - 10),\n",
    "                 cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "    return center_identified_img, cx, cy\n",
    "\n",
    "def contour_extraction(image, contours_space, cnt):\n",
    "    mask = np.zeros_like(image) # Create mask where white is what we want, black otherwise\n",
    "    cv2.drawContours(mask, contours_space, cnt, 255, -1) # Draw filled contour in mask\n",
    "    masked = np.zeros_like(image) # Extract out the object and place into output image\n",
    "    masked[mask == 255] = image[mask == 255]\n",
    "    return masked\n",
    "\n",
    "def contour_bound_crop(img, cnts, cnt):\n",
    "    [x,y,w,h] = cv2.boundingRect(cnts[cnt])\n",
    "    box = cv2.rectangle(img.copy(), (x, y), (x + w, y + h), (0,0,255), 2)\n",
    "    cropped = img[y-10:y+h+10, x-10:x+w+10]\n",
    "    return cropped, box\n",
    "\n",
    "def sharpen_image(image, sharpness, threshold):\n",
    "    \"\"\"Return a sharpened version of the image, using an unsharp mask.\"\"\"\n",
    "    blurred = cv2.GaussianBlur(image, (5,5) , 1.0)\n",
    "    sharpened = float(sharpness + 1) * image - float(sharpness) * blurred\n",
    "    sharpened = np.maximum(sharpened, np.zeros(sharpened.shape))\n",
    "    sharpened = np.minimum(sharpened, 255 * np.ones(sharpened.shape))\n",
    "    sharpened = sharpened.round().astype(np.uint8)\n",
    "    if threshold > 0:\n",
    "        low_contrast_mask = np.absolute(image - blurred) < threshold\n",
    "        np.copyto(sharpened, image, where=low_contrast_mask)\n",
    "    return sharpened\n",
    "\n",
    "def contour_properties(contour_selected, req):\n",
    "    perimeter = cv2.arcLength(contour_selected, True)                                                                    # perimeter of contour c (curved length)     \n",
    "    area = cv2.contourArea(contour_selected)                                                                     \n",
    "    corners = len(cv2.approxPolyDP(contour_selected, 0.04*perimeter, True))\n",
    "    if req == \"A\":\n",
    "        return area\n",
    "    elif req == \"P\":\n",
    "        return perimeter\n",
    "    elif req == \"C\":\n",
    "        return corners\n",
    "\n",
    "def shape_ratio(perimeter, area):\n",
    "    ratio = area/perimeter   \n",
    "    return ratio\n",
    "\n",
    "def fit_ellipse(contour_selected):\n",
    "    ellipse_found = cv2.fitEllipse(contour_selected)                                                                      # fit an ellipse on the contour\n",
    "    (center, axes, orientation) = ellipse_found                                                            # extract the main parameter\n",
    "    majoraxis_length = max(axes)                              \n",
    "    minoraxis_length = min(axes)\n",
    "    return minoraxis_length, majoraxis_length\n",
    "\n",
    "def shape_height_width(contour_selected, val):\n",
    "    minor, major = fit_ellipse(contour_selected)\n",
    "    if val == \"H\":\n",
    "        return major/minor\n",
    "    if val == \"W\":\n",
    "        return minor/major\n",
    "\n",
    "def contour_masking(morphology, cnts, selected):\n",
    "    mask = np.zeros_like(morphology)                                                               # Create mask where white is what we want, black otherwise\n",
    "    cv2.drawContours(mask, cnts, selected, 255, -1)                                       # Draw filled contour in mask\n",
    "    masked = np.zeros_like(morphology)                                                              #Extract out the object and place into output image\n",
    "    masked[mask == 255] = morphology[mask == 255]\n",
    "    return masked\n",
    "\n",
    "def harris_method_corners(image):\n",
    "    harris_method_BGR_img= image.copy()\n",
    "    harris_method_bw_img = np.float32(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY))\n",
    "    dst = cv2.cornerHarris(harris_method_bw_img,5,3,0.04)\n",
    "    ret, dst = cv2.threshold(dst,0.1*dst.max(),255,0)\n",
    "    dst = np.uint8(dst)\n",
    "    ret, labels, stats, centroids = cv2.connectedComponentsWithStats(dst)\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.001)\n",
    "    harris_corners = cv2.cornerSubPix(harris_method_bw_img,np.float32(centroids),(5,5),(-1,-1),criteria)\n",
    "    harris_method_BGR_img[dst>0.1*dst.max()]=[0,0,255]   \n",
    "    \n",
    "    for c in range(1, len(harris_corners)):\n",
    "        cv2.circle(harris_method_BGR_img, (int(harris_corners[c,0]), int(harris_corners[c,1])), 7, (0,255,0), 2)\n",
    "    \n",
    "    return c, harris_method_BGR_img\n",
    "\n",
    "def shi_tomasi_method_corners(image):\n",
    "    shi_tomasi_BGR_img= image.copy()\n",
    "    shi_tomasi_method_bw_img = cv2.cvtColor(shi_tomasi_BGR_img, cv2.COLOR_BGR2GRAY)\n",
    "    shi_tomasi_corners = cv2.goodFeaturesToTrack(shi_tomasi_method_bw_img, 0, 0.25, 0.05)\n",
    "    shi_tomasi_corners = np.int0(shi_tomasi_corners)\n",
    "                                                                                                  \n",
    "    for i in shi_tomasi_corners:                                                                                    # draw red color circles on all corners\n",
    "        x, y = i.ravel()\n",
    "        cv2.circle(shi_tomasi_BGR_img, (x, y), 3, (0, 0, 255), -1)\n",
    "    return len(shi_tomasi_corners), shi_tomasi_BGR_img\n",
    "\n",
    "def houghcircles(image, min_radius, max_radius, circle_dist):\n",
    "    ___, center_x, center_y = find_center(image)\n",
    "    circles_count = 0   \n",
    "    houghcircles_BGR_img= image.copy()\n",
    "    houghcircles_bw_img = cv2.cvtColor(houghcircles_BGR_img, cv2.COLOR_BGR2GRAY)\n",
    "    rows = houghcircles_bw_img.shape[0]\n",
    "    circles = cv2.HoughCircles(houghcircles_bw_img, cv2.HOUGH_GRADIENT, 1, rows / circle_dist,\n",
    "                               param1=100, param2=30, minRadius=min_radius, maxRadius=max_radius)\n",
    "    \n",
    "    dist_center_circle = 0\n",
    "    if circles is not None:\n",
    "        circles = np.uint16(np.around(circles))\n",
    "        for i in circles[0, :]:\n",
    "            center = (i[0], i[1]) \n",
    "            dist_center_circle += int(calculateDistance(center[0],center[1],center_x,center_y))\n",
    "            cv2.circle(houghcircles_BGR_img, center, 1, (0, 100, 100), 3)                           # circle center           \n",
    "            radius = i[2]                                                                              # circle outline\n",
    "            cv2.circle(houghcircles_BGR_img, center, radius, (255, 0, 255), 3)\n",
    "        circles_count = int(circles.size/3)\n",
    "    return circles_count, houghcircles_BGR_img, dist_center_circle\n",
    "\n",
    "def houghlines(image, canny_edges):\n",
    "    lines_img = image.copy()\n",
    "    rho = 1  # distance resolution in pixels of the Hough grid\n",
    "    theta = np.pi / 10  # angular resolution in radians of the Hough grid\n",
    "    threshold = 17 # minimum number of votes (intersections in Hough grid cell)\n",
    "    min_line_length = 15  # minimum number of pixels making up a line\n",
    "    max_line_gap = 18 # maximum gap in pixels between connectable line segments\n",
    "    line_image = np.copy(image) * 0  # creating a blank to draw lines on\n",
    "\n",
    "    # Run Hough on edge detected image\n",
    "    # Output \"lines\" is an array containing endpoints of detected line segments\n",
    "    lines = cv2.HoughLinesP(canny_edges, rho, theta, threshold, np.array([]),\n",
    "                        min_line_length, max_line_gap)\n",
    "    line_counter = 0\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            for x1,y1,x2,y2 in line:\n",
    "                line_counter += 1\n",
    "                cv2.line(line_image,(x1,y1),(x2,y2),(0,0,255),5)\n",
    "\n",
    "        lines_edges = cv2.addWeighted(lines_img, 1, line_image, 1, 0)\n",
    "\n",
    "        parallel = []\n",
    "        for i in range(len(lines)):\n",
    "            for j in range(len(lines)):\n",
    "                if (i == j):continue\n",
    "                if (abs(lines[i][0][1] - lines[j][0][1]) == 0):         \n",
    "                    parallel.append((i,j))\n",
    "        return line_counter, lines_edges, len(parallel)\n",
    "    \n",
    "def center_shape(img_frame, b, g, r):\n",
    "    mask = np.zeros_like(img_frame)\n",
    "    rows, cols,_ = mask.shape\n",
    "    center_x = int(rows/2)\n",
    "    center_y = int(cols/2)\n",
    "    center = (center_y,center_x)\n",
    "\n",
    "    mask = cv2.ellipse(mask, center, axes=(40, 60), angle=24, startAngle=0, endAngle = 360, color=(255,255,255), thickness = -1)\n",
    "    ellipse_masked_img = np.bitwise_and(img_frame, mask)\n",
    "    ellipse_masked_img[np.all(ellipse_masked_img == (0, 0, 0), axis=-1)] = (b, g, r)\n",
    "\n",
    "    return ellipse_masked_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04a948e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(cntr, colour_type):\n",
    "    try:\n",
    "        x, y, w, h = cv2.boundingRect(cntr)                                                     #find the x,y of origin, width and height of the contour\n",
    "        processing_frame = frame[y:y+h, x:x+w]\n",
    "        b,g,r = (frame[int((y+h+270)/2), int((x+w+70)/2)])\n",
    "        center_frame = center_shape(processing_frame, b,g,r) \n",
    "                \n",
    "        contours_uno, hierarchy_uno, canny_uno_img = find_contours(processing_frame, \"TREE\")\n",
    "        contours_center, hierarchy_center, canny_center_img = find_contours(center_frame, \"EXTERNAL\")\n",
    "    \n",
    "        contours_final = list()\n",
    "        hierarchy_center = hierarchy_center[0]\n",
    "        cut = 0\n",
    "        for component in zip(contours_center, hierarchy_center):\n",
    "            currentContour = component[0]\n",
    "            currentHierarchy = component[1]\n",
    "            if currentHierarchy[2] < 0:\n",
    "                contours_final.append(currentContour)\n",
    "                cut += 1\n",
    "\n",
    "        #defining all features\n",
    "        height_found, width_found, area_found, perimeter_found, poly_corners_found = 0,0,0,0,0\n",
    "        for c in range(len(contours_final)):\n",
    "            area_found += contour_properties(contours_final[c], \"A\")\n",
    "            perimeter_found += contour_properties(contours_final[c], \"P\")\n",
    "            poly_corners_found += contour_properties(contours_final[c], \"C\")\n",
    "            height_found += shape_height_width(contours_final[c], \"H\") \n",
    "            width_found += shape_height_width(contours_final[c], \"W\") \n",
    "\n",
    "        ratio_found = shape_ratio(perimeter_found, area_found)    \n",
    "        harris_corners, harris_corners_img = harris_method_corners(center_frame)\n",
    "        shi_tomasi_corners, shi_tomasi_img = shi_tomasi_method_corners(center_frame)\n",
    "        lines_found, lines_found_image, parallel_instances_found = houghlines(center_frame, canny_center_img)\n",
    "        circles_found, houghcircles_img, distance_circles = houghcircles(center_frame, 20, 30, 8)\n",
    "        children_found_counter, children_found_img = yougest_children_img(processing_frame, contours_uno, hierarchy_uno)\n",
    "\n",
    "        features = [int(-(-ratio_found // 1)), float(\"{:.1f}\".format(height_found)), float(\"{:.1f}\".format(width_found)), \n",
    "                    children_found_counter, lines_found, parallel_instances_found, circles_found, distance_circles, harris_corners, shi_tomasi_corners, poly_corners_found]\n",
    "        features_array = (np.array(features)).reshape(1, -1)\n",
    "\n",
    "        result = int(rfc.predict(features_array))\n",
    "        text = colour_type + str(result) + \" UNO CARD\"                                                            #if area is higher than area_sensitivity (perfect value for my camera = 8000)\n",
    "        return text, canny_center_img, x, y, w, h\n",
    "    \n",
    "    except (TypeError, IndexError, ZeroDivisionError, cv2.error) as e:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f45dc43f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-5049dd74c95a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marea_red\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0marea_sensitivity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m                                                                   \u001b[1;31m#if area is higher than area_sensitivity (perfect value for my camera = 8000)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[0mresult_red\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanny_center_red\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_red\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_red\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_red\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_red\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontour_red\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"RED \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-8daa2928c4af>\u001b[0m in \u001b[0;36mprocess_frame\u001b[1;34m(cntr, colour_type)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mshi_tomasi_corners\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshi_tomasi_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshi_tomasi_method_corners\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcenter_frame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mlines_found\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlines_found_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparallel_instances_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhoughlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcenter_frame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanny_center_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mcircles_found\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhoughcircles_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistance_circles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhoughcircles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcenter_frame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0mchildren_found_counter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchildren_found_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0myougest_children_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessing_frame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontours_uno\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhierarchy_uno\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------------------CAMERA SETUP-------------------------------------------------------------------------------------------\n",
    "camera = cv2.VideoCapture(0)                                                                           #setting camera 0 as our image input\n",
    "camera.set(cv2.CAP_PROP_FRAME_WIDTH, 800)                                                              #setting camera width resolution as 800\n",
    "camera.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)                                                             #setting camera height resolution as 480\n",
    "area_sensitivity = 8000                                                                                #area threshold to detect items\n",
    "\n",
    "rfc = pickle.load(open(\"randomforest_clf_optimised.p\", \"rb\"))                                    #load trained model with the pickle library\n",
    "#---------------------------------------------------------------------------------------CAMERA INPUT PROCESSING-------------------------------------------------------------------------------------\n",
    "while True:                                                                                            #create a while loop to check for camera input\n",
    "    _, frame = camera.read()                                                                           #reading the video from the camera in image frames\n",
    "    #frame = cv2.flip(frame,1)\n",
    "    hsvFrame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)                                                  #convert the colour space from BGR to HSV for further processing\n",
    "    kernel = np.ones((5,5), \"uint8\")                                                                   #create a kernel to be used for dilation in order to remove noise/holes from the frame\n",
    "    \n",
    "#----------------------------------------------------------------------------------------RED COLOUR PROCESSING-------------------------------------------------------------------------------------\n",
    "    red_lower = np.array([136, 127, 111], np.uint8)                                                     #set the lower HSV range for the RED colour\n",
    "    red_upper = np.array([180, 255, 255], np.uint8)                                                    #set the upper HSV range for the RED colour\n",
    "    red_mask = cv2.inRange(hsvFrame, red_lower, red_upper)                                             #create a mask containing only the RED pixels from the original frame\n",
    "    red_mask = cv2.dilate(red_mask, kernel)                                                            #dilate the red mask and remove the noise by using the kernel\n",
    "    res_red = cv2.bitwise_and(frame, frame, mask = red_mask)                                           #merge the frames based on the red mask pixel coordinates\n",
    "    \n",
    "    contours_red, hierarchy_red = cv2.findContours(red_mask, cv2.RETR_TREE,                            #find the contours on the red mask\n",
    "                                                   cv2.CHAIN_APPROX_SIMPLE)           \n",
    "    for i_red, contour_red in enumerate(contours_red):                                                  #go through all the contours\n",
    "        area_red = cv2.contourArea(contour_red)                                                                #calculate area of the current contour\n",
    "        if(area_red > area_sensitivity):                                                                   #if area is higher than area_sensitivity (perfect value for my camera = 8000)   \n",
    "            try:\n",
    "                result_red, canny_center_red, x_red, y_red, w_red, h_red = process_frame(contour_red, \"RED \")\n",
    "            except TypeError:\n",
    "                continue\n",
    "            cv2.imshow(\"canny_center_red\", canny_center_red)\n",
    "            frame = cv2.rectangle(frame, (x_red, y_red), (x_red + w_red, y_red + h_red), (0, 0, 255), 2)                       #draw a rectangle around the detected image with the RED colour\n",
    "            cv2.putText(frame, result_red, (x_red, y_red), cv2.FONT_HERSHEY_SIMPLEX,                       #create text \"RED UNO CARD\" at the origin x,y of the detected contour\n",
    "                                                   0.7, (0, 0, 255), 2, cv2.LINE_AA)    \n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------------BLUE COLOUR PROCESSING-------------------------------------------------------------------------------------  \n",
    "    blue_lower = np.array([97, 150, 80], np.uint8)                                                     #set the lower HSV range for the BLUE colour\n",
    "    blue_upper = np.array([117, 255, 255], np.uint8)                                                   #set the upper HSV range for the BLUE colour\n",
    "    blue_mask = cv2.inRange(hsvFrame, blue_lower, blue_upper)                                          #create a mask containing only the BLUE pixels from the original frame\n",
    "    blue_mask = cv2.dilate(blue_mask, kernel)                                                          #dilate the red mask and remove the noise by using the kernel\n",
    "    res_blue = cv2.bitwise_and(frame, frame, mask = blue_mask)                                         #merge the frames based on the red mask pixel coordinates\n",
    "    \n",
    "    contours_blue, hierarchy_blue = cv2.findContours(blue_mask, cv2.RETR_TREE,                         #find the contours on the blue mask\n",
    "                                                      cv2.CHAIN_APPROX_SIMPLE)          \n",
    "    for cnt, contour in enumerate(contours_blue):                                                      #go through all the contours\n",
    "        area = cv2.contourArea(contour)                                                                #calculate area of the current contour\n",
    "        if(area > area_sensitivity):                                                                   #if area is higher than area_sensitivity (perfect value for my camera = 8000)\n",
    "            x, y, w, h = cv2.boundingRect(contour)                                                     #find the x,y of origin, width and height of the contour\n",
    "            frame = cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)                       #draw a rectangle around the detected image with the BLUE colour\n",
    "            cv2.putText(frame, \"BLUE UNO CARD\", (x, y), cv2.FONT_HERSHEY_SIMPLEX,                      #create text \"BLUE UNO CARD\" at the origin x,y of the detected contour\n",
    "                                                0.7, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "    \n",
    "#--------------------------------------------------------------------------------------YELLOW COLOUR PROCESSING-------------------------------------------------------------------------------------   \n",
    "    yellow_lower = np.array([23, 41, 133], np.uint8)                                                   #set the lower HSV range for the YELLOW colour\n",
    "    yellow_upper = np.array([23, 150, 255], np.uint8)                                                  #set the upper HSV range for the YELLOW colour\n",
    "    yellow_mask = cv2.inRange(hsvFrame, yellow_lower, yellow_upper)                                    #create a mask containing only the YELLOW pixels from the original frame\n",
    "    yellow_mask = cv2.dilate(yellow_mask, kernel)                                                      #dilate the red mask and remove the noise by using the kernel\n",
    "    res_yellow = cv2.bitwise_and(frame, frame, mask = yellow_mask)                                     #merge the frames based on the red mask pixel coordinates    \n",
    "            \n",
    "    contours_yellow, hierarchy_yellow = cv2.findContours(yellow_mask, cv2.RETR_TREE,                   #find the contours on the yellow mask\n",
    "                                                            cv2.CHAIN_APPROX_SIMPLE)        \n",
    "    for cnt, contour in enumerate(contours_yellow):                                                    #go through all the contours\n",
    "        area = cv2.contourArea(contour)                                                                #calculate area of the current contour\n",
    "        if(area > area_sensitivity):                                                                   #if area is higher than area_sensitivity (perfect value for my camera = 8000)\n",
    "            x, y, w, h = cv2.boundingRect(contour)                                                     #find the x,y of origin, width and height of the contour             \n",
    "            frame = cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 255), 2)                     #draw a rectangle around the detected image with the YELLOW colour\n",
    "            cv2.putText(frame, \"YELLOW UNO CARD\", (x, y), cv2.FONT_HERSHEY_SIMPLEX,                    #create text \"YELLOW UNO CARD\" at the origin x,y of the detected contour\n",
    "                                                0.7, (0, 255, 255), 2, cv2.LINE_AA)      \n",
    "            \n",
    "#--------------------------------------------------------------------------------------BLACK COLOUR PROCESSING-------------------------------------------------------------------------------------   \n",
    "    \"\"\"\n",
    "    black_lower = np.array([0, 0, 20], np.uint8)                                                        #set the lower HSV range for the BLACK colour\n",
    "    black_upper = np.array([180, 255, 60], np.uint8)                                                    #set the upper HSV range for the BLACK colour\n",
    "    black_mask = cv2.inRange(hsvFrame, black_lower, black_upper)                                        #create a mask containing only the BLACK pixels from the original frame\n",
    "    black_mask = cv2.dilate(black_mask, kernel)                                                         #dilate the red mask and remove the noise by using the kernel\n",
    "    res_black = cv2.bitwise_and(frame, frame, mask = black_mask)                                        #merge the frames based on the red mask pixel coordinates    \n",
    "            \n",
    "    contours_black, hierarchy_black = cv2.findContours(black_mask, cv2.RETR_TREE,                       #find the contours on the black mask\n",
    "                                                         cv2.CHAIN_APPROX_SIMPLE)          \n",
    "    for cnt, contour in enumerate(contours_black):                                                      #go through all the contours\n",
    "        area = cv2.contourArea(contour)                                                                 #calculate area of the current contour\n",
    "        if(area > 5000):                                                                                #if area is higher than area_sensitivity (perfect value for my camera = 8000)\n",
    "            #contours = np.vstack([contours[cnt], contours[cnt]])\n",
    "            x, y, w, h = cv2.boundingRect(contour)                                                      #find the x,y of origin, width and height of the contour             \n",
    "            frame = cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 0), 2)                          #draw a rectangle around the detected image with the BLACK colour\n",
    "            cv2.putText(frame, \"BLACK UNO CARD\", (x, y), cv2.FONT_HERSHEY_SIMPLEX,                      #create text \"BLACK UNO CARD\" at the origin x,y of the detected contour\n",
    "                                                   0.7, (0, 0, 0), 2, cv2.LINE_AA)             \n",
    "    \"\"\"\n",
    "\n",
    "#----------------------------------------------------------------------------------------PROGRAM TERMINATION--------------------------------------------------------------------------------------\n",
    "    cv2.imshow(\"UNO Card Detection\", frame)                                                            #show the camera stream in a window called \"UNO Card Detection\"\n",
    "    #cv2.imshow('Red Mask', red_mask)\n",
    "    #cv2.imshow('Res Red', res_red)\n",
    "    if cv2.waitKey(10) == ord('q'):                                                                     #if the q key has been pressed then:\n",
    "        camera.release()                                                                                   #release the camera input\n",
    "        cv2.destroyAllWindows()                                                                            #close the window\n",
    "        break                                                                                              #exit the while loop\n",
    "       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
